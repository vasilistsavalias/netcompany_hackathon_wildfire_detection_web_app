# -*- coding: utf-8 -*-
"""notebook21bb4ff297

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook21bb4ff297-418059b2-e374-4ed7-8f50-fe39dfef66ea.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250215/auto/storage/goog4_request%26X-Goog-Date%3D20250215T105321Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da76b2814d1e8dbff4089f2fc8b7897ad646ac3876a9500ec7438428a13b24949de79b1881ea468790dadad0348ed8210f12c6a7e5427f4fde5ccd83052d425210d785b7d330c6c4506b562719923814fd8fdbd2a1466042a3daa695153a4c17e03d5f4e42eabe34c07dad827bc52fdf5104b21e2e6e12ff493353b2448b8d9c6d4be03b86efa3dfb045bb3e53a96c2a6f3fea76e32251d7aefd621e30a17ccbb46be125f7b9f742097f783eaa21eb3f8278e865f3c3aab77c55418d5fa37cb8e326f662b378282cab69d1aad4db07d18024b779e5240a5f1707b854fb363d5bd561bf10b71bb2ba590d3051a32bf90b705c7e3d980fd51f8bac32eeba8fb5a05
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

machinelearningmain_my_data_path = kagglehub.dataset_download('machinelearningmain/my-data')
machinelearningmain_cnn_keras_best_cnn_1_path = kagglehub.model_download('machinelearningmain/cnn/Keras/best_cnn/1')
machinelearningmain_yolov8_pytorch_best_yolov8_1_path = kagglehub.model_download('machinelearningmain/yolov8/PyTorch/best_yolov8/1')

print('Data source import complete.')

"""# See which architectures are available (and check premium proccessing units is been used )"""

!pip install ultralytics torch
import tensorflow as tf
import torch

# Check for GPU using TensorFlow
physical_devices = tf.config.list_physical_devices()
print("Available physical devices (TensorFlow):", physical_devices)

gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
    print("GPU is available (TensorFlow)")
else:
    print("GPU is not available (TensorFlow)")

# Check for GPU using PyTorch
if torch.cuda.is_available():
    print("GPU is available (PyTorch)")
    print("CUDA version:", torch.version.cuda)
    print("Number of GPUs:", torch.cuda.device_count())
    print("GPU name:", torch.cuda.get_device_name(0))
else:
    print("GPU is not available (PyTorch)")

# Check for TPU
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('TPU is available')
    print('TPU devices:', tpu.master())
except ValueError:
    print('TPU is not available')

# System information
print("System information:")
!lscpu

# GPU information
print("GPU information:")
!nvidia-smi

# Full hardware information
print("Full hardware information:")
!lshw -short



device_count = torch.cuda.device_count()
print(f"Number of available GPUs: {device_count}")

"""# See directories structure"""

import os

print("Current working directory:", os.getcwd())
print("Contents of root directory:", os.listdir('/'))
print("Contents of /kaggle:", os.listdir('/kaggle'))
print("Contents of /kaggle/input:", os.listdir('/kaggle/input'))

# Try accessing your dataset directory again
dataset_path = '/kaggle/input/dataset'
if os.path.exists(dataset_path):
    print("Dataset path verified:", dataset_path)
    print("Contents of dataset path:", os.listdir(dataset_path))
else:
    print("Dataset path does not exist:", dataset_path)

"""# DIRECTORY LISTING (3 FILES AT THE DEEPEST LEVEL)"""

import os

def list_directory_contents(path, level=0, max_items=3):
    if not os.path.exists(path):
        print(f"Path does not exist: {path}")
        return

    indent = '│   ' * level
    entries = os.listdir(path)
    entries.sort()

    dirs = [entry for entry in entries if os.path.isdir(os.path.join(path, entry))]
    files = [entry for entry in entries if os.path.isfile(os.path.join(path, entry))]

    # Print directories
    for i, d in enumerate(dirs):
        print(f"{indent}{'└──' if i == len(dirs) - 1 and not files else '├──'} {d}/")
        list_directory_contents(os.path.join(path, d), level + 1, max_items)

    # Print files, limit to max_items at the deepest level
    for j, f in enumerate(files[:max_items]):
        print(f"{indent}{'└──' if j == len(files[:max_items]) - 1 else '├──'} File: {f}")

list_directory_contents('/kaggle/input/my-data/dataset')

"""# Get rid of possible bugs"""

!pip install --user --ignore-installed --upgrade tensorflow

# used to force upgrade tensorflow in user system,ignoring old dependecies and forcing a clean install of the newest version

"""# Check if keras is sucessfully installed"""

# Verify TensorFlow Installation
import tensorflow as tf
print(f"TensorFlow version: {tf.__version__}")

try:
    from tensorflow.keras.models import Model
    print("tensorflow.keras is available")
except ModuleNotFoundError as e:
    print("tensorflow.keras is NOT available:", e)

"""# CNN MODEL

# Imports and Setup
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.applications import ResNet50, InceptionV3, Xception
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.metrics import Precision, Recall, AUC
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score, confusion_matrix, classification_report
import numpy as np
import optuna
import os
import time
import logging
import json


# Enable AMP
tf.keras.mixed_precision.set_global_policy('mixed_float16')



input_shape = (224, 224, 3)


def setup_environment():
    """
    Set up the environment for the execution.
    """
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)
    global dataset_path, output_path
    dataset_path = '/kaggle/input/my-dataset/dataset_image_classification'
    output_path = '/kaggle/working/wildfire_detection_output'
    os.makedirs(output_path, exist_ok=True)
    logging.info("Environment setup completed.")

"""Data loading"""

def load_data():
    """
    Load and preprocess the data.

    Returns:
        tuple: Train, validation, and test datasets along with class weight dictionary.
    """
    global input_shape  # Declare the global variable to use it within the function
    train_dataset, val_dataset, test_dataset, class_weight_dict = load_preproccess_data_cnn(dataset_path, input_shape)
    logging.info("Data loaded and preprocessed.")
    return train_dataset, val_dataset, test_dataset, class_weight_dict

"""#  Preprocessing with data augmentation"""

def combined_generator(gen1, gen2):
    while True:
        yield next(gen1)
        yield next(gen2)

def load_preproccess_data_cnn(dataset_path, input_shape):
    """
    Load and preprocess the dataset with data augmentation for both classes.

    Args:
        dataset_path (str): Path to the dataset directory.
        input_shape (tuple): Shape of the input images.

    Returns:
        tuple: Train, validation, and test datasets along with class weight dictionary.
    """
    logging.info("Loading and preprocessing the dataset...")

    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    test_datagen = ImageDataGenerator(rescale=1./255)

    train_dataset = train_datagen.flow_from_directory(
        os.path.join(dataset_path, 'train'),
        target_size=input_shape[:2],
        batch_size=32,
        class_mode='binary',  # Ensure binary class labels (0 and 1)
        seed=42
    )

    val_dataset = test_datagen.flow_from_directory(
        os.path.join(dataset_path, 'val'),
        target_size=input_shape[:2],
        batch_size=32,
        class_mode='binary'  # Ensure binary class labels (0 and 1)
    )

    test_dataset = test_datagen.flow_from_directory(
        os.path.join(dataset_path, 'test'),
        target_size=input_shape[:2],
        batch_size=32,
        class_mode='binary'  # Ensure binary class labels (0 and 1)
    )

    # Compute class weights
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=np.unique(train_dataset.classes),
        y=train_dataset.classes
    )
    class_weight_dict = dict(enumerate(class_weights))

    logging.info("Dataset loaded and preprocessed.")
    return train_dataset, val_dataset, test_dataset, class_weight_dict

"""# Model Creation"""

# Cell 3: Model Creation
def create_model_cnn(params):
    """
    Create the CNN model with the given hyperparameters.

    Args:
        params (dict): Hyperparameters for the model.

    Returns:
        Model: Compiled CNN model.
    """
    logging.info("Creating the CNN model with params: %s", params)
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    x = base_model.output
    x = GlobalAveragePooling2D()(x)

    dense_units = params.get('dense_units', 256)
    x = Dense(dense_units, activation='relu')(x)

    output = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=base_model.input, outputs=output)
    for layer in base_model.layers:
        layer.trainable = False

    learning_rate = params.get('learning_rate', 1e-4)
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])
    logging.info("CNN model created.")
    return model

"""# Objective function"""

def train_and_evaluate_cnn(model, train_dataset, val_dataset, params):
    """
    Train and evaluate the CNN model.

    Args:
        model (Model): Compiled CNN model.
        train_dataset (DirectoryIterator): Training dataset.
        val_dataset (DirectoryIterator): Validation dataset.
        params (dict): Hyperparameters for training.

    Returns:
        tuple: Trained model, training history, elapsed time, and validation metrics.
    """
    logging.info("Training the model...")
    early_stopping = EarlyStopping(monitor='val_loss', patience=params['patience'], restore_best_weights=True)

    start_time = time.time()
    history = model.fit(train_dataset,
                        epochs=params['epochs'],
                        validation_data=val_dataset,
                        callbacks=[early_stopping])
    elapsed_time = time.time() - start_time

    val_loss, val_accuracy, val_precision, val_recall, val_auc = model.evaluate(val_dataset)

    logging.info(f"Validation metrics: Loss={val_loss:.4f}, Accuracy={val_accuracy:.4f},\
    Precision={val_precision:.4f}, Recall={val_recall:.4f}, AUC={val_auc:.4f}")
    logging.info(f"Training time: {elapsed_time:.2f} seconds")

    metrics = {
        'val_loss': val_loss,
        'val_accuracy': val_accuracy,
        'val_precision': val_precision,
        'val_recall': val_recall,
        'val_auc': val_auc,
        'elapsed_time': elapsed_time
    }

    return model, history, metrics

"""# Objective Function for Hyperparameter Tuning"""

def objective_ensemble(trial, train_dataset, val_dataset, class_weight_dict):
    """
    Objective function for hyperparameter tuning with Optuna.

    Args:
        trial (optuna.trial.Trial): Optuna trial object.
        train_dataset (DirectoryIterator): Training dataset.
        val_dataset (DirectoryIterator): Validation dataset.
        class_weight_dict (dict): Dictionary of class weights.

    Returns:
        float: Validation F1 score.
    """
    print("Starting a new Optuna trial...")
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
        'dense_units': trial.suggest_categorical('dense_units', [128, 256, 512]),
        'dropout_rate': trial.suggest_float('dropout_rate', 0.3, 0.7),
        'epochs': trial.suggest_int('epochs', 20, 40),
        'patience': trial.suggest_int('patience', 5, 15),
        'batch_size': trial.suggest_categorical('batch_size', [16, 32])
    }

    model = create_ensemble_model(params)
    print("Training the model...")
    model, history, metrics = train_and_evaluate_ensemble(model, train_dataset, val_dataset, params)

    val_f1 = metrics['val_f1']
    print(f"Validation F1 score: {val_f1:.4f}")

    return val_f1

"""# Model Saving and Information Logging"""

# Cell 6: Model saving and information logging
def save_model_info_cnn(model, metrics, best_params, output_path):
    """
    Save the model, its information, and the best hyperparameters.

    Args:
        model (Model): Trained CNN model.
        metrics (dict): Validation metrics.
        best_params (dict): Best hyperparameters found during tuning.
        output_path (str): Output directory path.
    """
    logging.info("Saving the model, its information, and the best hyperparameters...")
    best_cnn_path = os.path.join(output_path, 'best_cnn')
    os.makedirs(best_cnn_path, exist_ok=True)

    # Save the model
    model.save(os.path.join(best_cnn_path, 'best_model.keras'))

    # Save the model structure
    with open(os.path.join(best_cnn_path, 'model_structure.txt'), 'w') as f:
        model.summary(print_fn=lambda x: f.write(x + '\n'))

    # Save the metrics
    with open(os.path.join(best_cnn_path, 'metrics.yaml'), 'w') as f:
        yaml.safe_dump(metrics, f)

    # Save the best hyperparameters
    with open(os.path.join(best_cnn_path, 'best_params.yaml'), 'w') as f:
        yaml.safe_dump(best_params, f)

    logging.info("Model, its information, and the best hyperparameters saved.")

"""# hyperparameter tuning"""

def tune_hyperparameters(train_dataset, val_dataset, class_weight_dict):
    """
    Tune hyperparameters using Optuna.

    Args:
        train_dataset (DirectoryIterator): Training dataset.
        val_dataset (DirectoryIterator): Validation dataset.
        class_weight_dict (dict): Class weights dictionary.

    Returns:
        dict: Best hyperparameters found.
    """
    logging.info("Creating Optuna study...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective_ensemble(trial, train_dataset, val_dataset, class_weight_dict), n_trials=3)
    logging.info(f"Best hyperparameters: {study.best_params}")
    return study.best_params

"""# Create ensemble

# train and validate the ensemble model
"""

def train_and_validate(train_dataset, val_dataset, best_params):
    """
    Train and validate the ensemble model.

    Args:
        train_dataset (DirectoryIterator): Training dataset.
        val_dataset (DirectoryIterator): Validation dataset.
        best_params (dict): Best hyperparameters found.

    Returns:
        tuple: Trained model and metrics.
    """
    best_model = create_ensemble_model(best_params)
    best_model, history, metrics = train_and_evaluate_ensemble(best_model, train_dataset, val_dataset, best_params)

    os.makedirs(os.path.join(output_path, 'final_model'), exist_ok=True)
    with open(os.path.join(output_path, 'final_model', 'training_history.json'), 'w') as f:
        json.dump(history.history, f)

    logging.info("Model trained and validated.")
    return best_model, metrics

"""# Train and evalute the ensembled model"""

def train_and_evaluate_ensemble(model, train_dataset, val_dataset, params):
    """
    Train and evaluate the ensemble model.

    Args:
        model (Model): Ensemble model to train and evaluate.
        train_dataset (DirectoryIterator): Training dataset.
        val_dataset (DirectoryIterator): Validation dataset.
        params (dict): Hyperparameters for training.

    Returns:
        tuple: Trained model, training history, and evaluation metrics.
    """
    logging.info("Training the ensemble model...")
    early_stopping = EarlyStopping(monitor='val_loss', patience=params['patience'], restore_best_weights=True)

    start_time = time.time()
    history = model.fit(train_dataset,
                        epochs=params['epochs'],
                        validation_data=val_dataset,
                        callbacks=[early_stopping])
    elapsed_time = time.time() - start_time

    logging.info("Evaluating the ensemble model...")
    val_loss, val_accuracy, val_precision, val_recall, val_auc = model.evaluate(val_dataset)

    val_predictions = (model.predict(val_dataset) > 0.5).astype(int)
    val_labels = val_dataset.classes
    val_f1 = f1_score(val_labels, val_predictions)

    metrics = {
        'val_loss': val_loss,
        'val_accuracy': val_accuracy,
        'val_precision': val_precision,
        'val_recall': val_recall,
        'val_auc': val_auc,
        'val_f1': val_f1,
        'elapsed_time': elapsed_time
    }

    logging.info("Ensemble model trained and evaluated.")
    return model, history, metrics

"""# Evaluate final on test"""

def evaluate_model(best_model, test_dataset):
    """
    Evaluate the model on the test set.

    Args:
        best_model (Model): Trained ensemble model.
        test_dataset (DirectoryIterator): Test dataset.
    """
    logging.info("Evaluating the final model on the test set...")
    test_predictions = best_model.predict(test_dataset)
    test_labels = test_dataset.classes
    np.save(os.path.join(output_path, 'final_model', 'test_predictions.npy'), test_predictions)
    np.save(os.path.join(output_path, 'final_model', 'test_labels.npy'), test_labels)

    logging.info("Saving the final model...")
    final_model_path = os.path.join(output_path, 'final_model')
    os.makedirs(final_model_path, exist_ok=True)
    best_model.save(os.path.join(final_model_path, 'final_model.keras'))
    logging.info("Model evaluation completed and model saved.")

"""# Analyzing and Interpreting the Results"""

def analyze_results(metrics, test_dataset, best_model):
    """
    Analyze and interpret the model's performance metrics.

    Args:
        metrics (dict): Evaluation metrics of the model.
        test_dataset (DirectoryIterator): Test dataset.
        best_model (Model): Best trained model.
    """
    logging.info("Analyzing and interpreting the results...")

    logging.info("Validation Metrics:")
    logging.info(f"  Accuracy: {metrics['val_accuracy']:.4f}")
    logging.info(f"  Precision: {metrics['val_precision']:.4f}")
    logging.info(f"  Recall: {metrics['val_recall']:.4f}")
    logging.info(f"  F1 Score: {metrics['val_f1']:.4f}")
    logging.info(f"  AUC: {metrics['val_auc']:.4f}")
    logging.info(f"  Elapsed Time: {metrics['elapsed_time']:.2f} seconds")

    logging.info("Interpreting the results:")
    if metrics['val_f1'] >= 0.80:
        logging.info("  The model achieved a high F1 score, indicating strong performance in classifying wildfire images.")
    else:
        logging.info("  The model's F1 score can be further improved. Consider adjusting the architecture or hyperparameters.")

    logging.info("Evaluating the model on the test set...")
    test_loss, test_accuracy, test_precision, test_recall, test_auc = best_model.evaluate(test_dataset)
    test_predictions = (best_model.predict(test_dataset) > 0.5).astype(int)
    test_labels = test_dataset.classes
    test_f1 = f1_score(test_labels, test_predictions)

    logging.info("Test Metrics:")
    logging.info(f"  Accuracy: {test_accuracy:.4f}")
    logging.info(f"  Precision: {test_precision:.4f}")
    logging.info(f"  Recall: {test_recall:.4f}")
    logging.info(f"  F1 Score: {test_f1:.4f}")
    logging.info(f"  AUC: {test_auc:.4f}")

    logging.info("Confusion Matrix:")
    cm = confusion_matrix(test_labels, test_predictions)
    logging.info(cm)

    logging.info("Classification Report:")
    cr = classification_report(test_labels, test_predictions)
    logging.info(cr)

    logging.info("Results analyzed and interpreted.")

"""#  Main execution"""

def main_execution():
    """
    Main execution function to set up environment, load data, tune hyperparameters,
    train, validate, and evaluate the model.
    """
    setup_environment()

    train_dataset, val_dataset, test_dataset, class_weight_dict = load_data()

    best_params = tune_hyperparameters(train_dataset, val_dataset, class_weight_dict)

    best_model, metrics = train_and_validate(train_dataset, val_dataset, best_params)

    evaluate_model(best_model, test_dataset)

    # Run the analysis
    analyze_results(metrics, test_dataset, best_model)
    logging.info("All done!")


# Run the main execution
if __name__ == "__main__":
    main_execution()

"""# Visualizing the CNN Performance"""

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve
from sklearn.metrics import ConfusionMatrixDisplay
import json

def plot_metric(history, metric, title, ylabel, output_dir):
    plt.figure()
    if f'{metric}' in history:
        plt.plot(history[f'{metric}'], label=f'Training {metric}')
    if f'val_{metric}' in history:
        plt.plot(history[f'val_{metric}'], label=f'Validation {metric}')
    plt.xlabel('Epochs')
    plt.ylabel(ylabel)
    plt.legend()
    plt.title(title)
    plot_path = os.path.join(output_dir, f"{metric}_plot.png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.show()

def generate_evaluation_plots(output_path):
    final_model_path = os.path.join(output_path, 'final_model')
    cnn_graphs_dir = os.path.join(output_path, 'cnn_graphs')
    os.makedirs(cnn_graphs_dir, exist_ok=True)

    # Check if the required files exist
    test_predictions_path = os.path.join(final_model_path, 'test_predictions.npy')
    test_labels_path = os.path.join(final_model_path, 'test_labels.npy')
    history_path = os.path.join(final_model_path, 'training_history.json')

    if not os.path.isfile(test_predictions_path):
        print(f"Error: {test_predictions_path} not found.")
        return
    if not os.path.isfile(test_labels_path):
        print(f"Error: {test_labels_path} not found.")
        return
    if not os.path.isfile(history_path):
        print(f"Error: {history_path} not found.")
        return

    try:
        test_predictions = np.load(test_predictions_path)
        test_labels = np.load(test_labels_path)
        test_predictions_binary = (test_predictions > 0.5).astype(int)
        cm = confusion_matrix(test_labels, test_predictions_binary)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Fire', 'Fire'])
        fig, ax = plt.subplots(figsize=(8, 6))
        disp.plot(cmap=plt.cm.Blues, ax=ax)
        plt.title('Confusion Matrix (Test Set)')
        plot_path = os.path.join(cnn_graphs_dir, "confusion_matrix.png")
        plt.savefig(plot_path, bbox_inches='tight')
        plt.show()

        fpr, tpr, _ = roc_curve(test_labels, test_predictions)
        roc_auc = auc(fpr, tpr)
        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (Test Set)')
        plt.legend(loc="lower right")
        plot_path = os.path.join(cnn_graphs_dir, "roc_curve.png")
        plt.savefig(plot_path, bbox_inches='tight')
        plt.show()

        precision, recall, _ = precision_recall_curve(test_labels, test_predictions)
        plt.figure()
        plt.plot(recall, precision, color='blue', lw=2)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve (Test Set)')
        plot_path = os.path.join(cnn_graphs_dir, "precision_recall_curve.png")
        plt.savefig(plot_path, bbox_inches='tight')
        plt.show()

        with open(history_path, 'r') as f:
            history_dict = json.load(f)
        plot_metric(history_dict, 'accuracy', 'Training and Validation Accuracy', 'Accuracy', cnn_graphs_dir)
        plot_metric(history_dict, 'loss', 'Training and Validation Loss', 'Loss', cnn_graphs_dir)
        plot_metric(history_dict, 'precision', 'Training and Validation Precision', 'Precision', cnn_graphs_dir)
        plot_metric(history_dict, 'recall', 'Training and Validation Recall', 'Recall', cnn_graphs_dir)
        plot_metric(history_dict, 'auc', 'Training and Validation AUC', 'AUC', cnn_graphs_dir)

    except Exception as e:
        print(f"Error: {e}")

# Example usage
output_path = '/kaggle/working/wildfire_detection_output'
generate_evaluation_plots(output_path)

"""# Yolo section

##  Dependecies
"""

!pip install --user --ignore-installed --upgrade tensorflow
# used to force upgrade tensorflow in user system,ignoring old dependecies and forcing a clean install of the newest version
   # Install necessary dependencie
!pip install tensorflow ultralytics
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt



# Verify TensorFlow Installation

print(f"TensorFlow version: {tf.__version__}")

try:
    from tensorflow.keras.models import Model
    print("tensorflow.keras is available")
except ModuleNotFoundError as e:
    print("tensorflow.keras is NOT available:", e)

"""## workaround to not get kaggle idle out"""

# # This Python 3 environment comes with many helpful analytics libraries installed
# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# # For example, here's several helpful packages to load in

# import numpy as np # linear algebra
# from IPython.display import HTML, display
# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# # Input data files are available in the "../input/" directory.
# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

# from subprocess import check_output

# htmlCodeHide="""
#  <script>
#    var divTag = document.getElementsByClassName("input")[1]
#    var displaySetting = divTag.style.display;
#    divTag.style.display = 'block';
#    </script>
#   <button onclick="javascript:toggleInput(1)" class="button">Hide Code</button>
# """

# print("Sample Output")

# #HTML(htmlCodeHide)
# HTML(""" <button onclick="javascript:toggleInput(1)" class="button">Hide Code</button>""")

"""# ADVANCED PERFORMANCE ANALYSIS - YOLO

# Setup and Imports
"""

!pip uninstall wandb -y  # Remove wandb completely
!pip install wandb

!pip install keras_preprocessing

import os
import logging
import json
import time
import numpy as np
from ultralytics import YOLO
import optuna
import wandb
import yaml
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt



#Set up wandb key
wandb.login(key='a4500a515d3341473dcd10cc91456942c6aeabb9')

# Setting up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

# Define paths
dataset_path = '/kaggle/input/my-data/dataset/dataset_object_detection'
output_path = '/kaggle/working/wildfire_detection_output'
os.makedirs(output_path, exist_ok=True)

def setup_environment():
    """
    Set up the environment for the execution.
    """
    logging.info("Environment setup completed.")

"""##  Training"""

def train_yolov8(params):
    """
    Train YOLOv8 model with given parameters.

    Args:
        params (dict): Hyperparameters for the model.

    Returns:
        model: Trained YOLOv8 model.
    """
    logging.info("Training YOLOv8 model with params: %s", params)
    # Load a pre-trained YOLOv8 model
    model = YOLO('yolov8n.pt')

    # Train the model with data augmentation
    model.train(
        data='/kaggle/input/my-data/dataset/dataset_object_detection/data.yaml',  # Path to data.yaml
        epochs=params['epochs'],  # Number of training epochs
        imgsz=params['imgsz'],  # Image size
        batch=params['batch_size'],  # Batch size
        name='yolov8_smoke_detection',  # Update the name to reflect the smoke detection task
        patience=10,  # Early stopping patience
        augment=True,  # Enable data augmentation
        mosaic=True,  # Enable mosaic augmentation
        hsv_h=0.015,  # Adjust hue augmentation
        hsv_s=0.7,  # Adjust saturation augmentation
        hsv_v=0.4,  # Adjust value augmentation
        flipud=0.5,  # Adjust vertical flip augmentation
        fliplr=0.5,  # Adjust horizontal flip augmentation
        single_cls=True  # Add this line to specify single-class training
    )
    # Check the class names
    class_names = model.names
    if len(class_names) != 1 or class_names[0] != 'smoke':
        raise ValueError(f"Model trained with incorrect class names: {class_names}")
    return model

"""## Evaluation"""

def evaluate_yolov8(model, dataset_type='val'):
    """
    Evaluate the YOLOv8 model on the validation/test dataset.

    Args:
        model: YOLOv8 model.
        dataset_type (str): Type of dataset to evaluate on ('val' or 'test').

    Returns:
        dict: Evaluation metrics.
    """
    logging.info(f"Evaluating the model on the {dataset_type} set...")
    results = model.val(data='/kaggle/input/my-data/dataset/dataset_object_detection/data.yaml', split=dataset_type)

    # Access the DetMetrics object
    metrics = results.box

    avg_metrics = {
        'val_precision': metrics.mean_results()[0],  # Mean precision
        'val_recall': metrics.mean_results()[1],     # Mean recall
        'val_mAP_0.5': metrics.map50,        # mAP@0.5
        'val_mAP_0.5_0.95':metrics.map,  # mAP@0.5:0.95
        'val_fitness': metrics.fitness,              # Fitness score
    }

    logging.info(f"Evaluation metrics: Precision: {avg_metrics['val_precision']}, "
                 f"Recall: {avg_metrics['val_recall']}, "
                 f"mAP@0.5: {avg_metrics['val_mAP_0.5']}, "
                 f"mAP@0.5:0.95: {avg_metrics['val_mAP_0.5_0.95']}, "
                 f"Fitness: {avg_metrics['val_fitness']}")

    return avg_metrics

"""##  objective ensembled"""

def objective_ensemble(trial):
    """
    Objective function for hyperparameter tuning with Optuna.

    Args:
        trial (optuna.trial.Trial): Optuna trial object.

    Returns:
        float: Validation mAP@0.5.
    """
    logging.info("Starting a new Optuna trial...")
    params = {
        'imgsz': trial.suggest_int('imgsz', 320, 640, step=32),
        'batch_size': trial.suggest_categorical('batch_size', [16, 32]),
        'epochs': trial.suggest_int('epochs', 20, 30)
    }

    model = train_yolov8(params)
    metrics = evaluate_yolov8(model)

    val_map_0_5 = metrics['val_mAP_0.5']
    logging.info(f"Validation mAP@0.5: {val_map_0_5:.4f}")

    return val_map_0_5

"""##  Hyperparameter tuning"""

def tune_hyperparameters(n_trials):
    """
    Tune hyperparameters using Optuna.

    Args:
        n_trials (int): Number of trials for Optuna optimization.

    Returns:
        dict: Best hyperparameters found.
    """
    logging.info("Creating Optuna study...")
    study = optuna.create_study(direction='maximize')
    study.optimize(objective_ensemble, n_trials=n_trials)
    logging.info(f"Best hyperparameters: {study.best_params}")
    return study.best_params

"""##  saving best model"""

def save_model_info(model, params, metrics, output_path):
    """
    Save the model along with its hyperparameters, metrics, and structure.

    Args:
        model: Trained YOLOv8 model.
        params (dict): Hyperparameters used for training.
        metrics (dict): Evaluation metrics.
        output_path (str): Path to save the model and info.
    """
    model_path = os.path.join(output_path, f'yolov8_model_{time.strftime("%Y%m%d-%H%M%S")}.pt')
    model.save(model_path)
    model_info = {
        'hyperparameters': params,
        'metrics': metrics,
        'model_structure': str(model),
        'class_names': model.names  # Add the class names to the model info
    }

    with open(os.path.join(output_path, 'model_info.json'), 'w') as f:
        json.dump(model_info, f)

    logging.info(f"Model and its information saved to {output_path}")

"""## ensemble_predictions"""

import numpy as np
import logging

def ensemble_predictions(models, dataset_type='test'):
    """
    Generate ensemble predictions from multiple models.

    Args:
        models (list): List of trained YOLOv8 models.
        dataset_type (str): Dataset type for evaluation ('val' or 'test').

    Returns:
        dict: Aggregated metrics from the ensemble.
    """
    metrics_list = []
    for model in models:
        metrics = evaluate_yolov8(model, dataset_type)
        metrics_list.append(metrics)

    # Initialize a dictionary to store aggregated metrics
    ensemble_metrics = {
        'val_precision': [],
        'val_recall': [],
        'val_mAP_0.5': [],
        'val_mAP_0.5_0.95': [],
        'val_fitness': []
    }

    # Collect each metric in a list if it's available and numeric
    for metrics in metrics_list:
        for key in ensemble_metrics.keys():
            value = metrics.get(key)
            if value is not None and isinstance(value, (int, float)):
                ensemble_metrics[key].append(value)
            else:
                logging.warning(f"Skipping non-numeric or missing metric '{key}' with value '{value}'")

    # Compute the mean of each metric
    for key in ensemble_metrics.keys():
        if ensemble_metrics[key]:
            ensemble_metrics[key] = np.mean(ensemble_metrics[key])
        else:
            ensemble_metrics[key] = None
            logging.warning(f"No valid entries found for metric '{key}'. Mean cannot be calculated.")

    logging.info(f"Ensemble evaluation metrics: {ensemble_metrics}")
    return ensemble_metrics

"""## Main execution"""

import logging

def main_execution():
    """
    Main execution function to set up environment, load data, tune hyperparameters,
    train, validate, and evaluate the model.
    """
    setup_environment()

    best_params = tune_hyperparameters(n_trials=3)

    # Train multiple models for ensembling
    models = []
    val_metrics_list = []
    best_model = None
    best_val_mAP = 0

    for i in range(3):  # Train 3 models for the ensemble
        model = train_yolov8(best_params)
        models.append(model)

        # Evaluate the model on the validation set
        val_metrics = evaluate_yolov8(model, 'val')
        val_metrics_list.append(val_metrics)

        # Check if this model is the best based on mAP@0.5
        if val_metrics['val_mAP_0.5'] > best_val_mAP:
            best_val_mAP = val_metrics['val_mAP_0.5']
            best_model = model

    test_metrics_list = [evaluate_yolov8(model, 'test') for model in models]

    val_metrics_ensemble = ensemble_predictions(models, 'val')
    test_metrics_ensemble = ensemble_predictions(models, 'test')

    logging.info("Validation Metrics:")
    logging.info(f"  {val_metrics_ensemble}")

    logging.info("Test Metrics:")
    logging.info(f"  {test_metrics_ensemble}")

    # Save the best model
    best_model_path = '/kaggle/working/wildfire_detection_output/best_yolov8_model.pt'
    best_model.save(best_model_path)

    # Save the best model's info
    save_model_info(best_model, best_params, test_metrics_ensemble, output_path)

    logging.info("All done!")

if __name__ == "__main__":
    main_execution()

"""# trying to match wandb runs with current best run"""

# # import wandb
# # import yaml
# import os
# import json

# # Authenticate with wandb
# wandb.login(key="a4500a515d3341473dcd10cc91456942c6aeabb9")

# # Define your project name
# project_name = "YOLOv8"

# # Path to the best metrics file
# best_metrics_path = "/kaggle/working/best_yolo/best_metrics.yaml"

# # Load the best metrics
# with open(best_metrics_path, 'r') as f:
#     best_metrics = yaml.safe_load(f)

# # Print loaded best metrics for verification
# print("Loaded Best Metrics:")
# print(json.dumps(best_metrics, indent=2))

# # Initialize a new wandb API client
# api = wandb.Api()

# # Fetch all runs from the project
# runs = api.runs(f"{api.default_entity}/{project_name}")

# # Initialize variables to track the best run
# best_run = None

# # Assuming you saved the metrics in a similar structure in wandb
# best_metric_value = best_metrics['best_metric']  # Adjust according to your actual metric structure
# best_params = best_metrics['best_params']  # Adjust according to your actual params structure

# # Debug: print fetched runs
# print(f"Fetched {len(runs)} runs from project {project_name}")

# # Set a tolerance for floating-point comparison
# tolerance = 1

# # Loop through the runs to find the one with matching metrics
# for run in runs:
#     if 'metrics/mAP50-95(B)' in run.summary:
#         metric_value = run.summary['metrics/mAP50-95(B)']
#         # Check if the metric value is close to the best metric value within the tolerance
#         if abs(metric_value - best_metric_value) < tolerance:
#             best_run = run
#             break

#     # Print run summaries for debugging
#     print(f"Run ID: {run.id}, Summary: {run.summary}")

# # Print the best run details
# if best_run:
#     print(f"Best run found: {best_run.name}")
#     print(f"Run ID: {best_run.id}")
#     print(f"Run Summary: {best_run.summary}")
# else:
#     print("No matching run found.")

# # Optional: Download the files from the best run
# if best_run:
#     best_run_dir = "/kaggle/working/best_wandb_run"
#     os.makedirs(best_run_dir, exist_ok=True)

#     # Download files from the best run
#     for file in best_run.files():
#         file.download(root=best_run_dir)

#     print(f"Best run files downloaded to {best_run_dir}")

# # Check if the best run metrics match those stored locally
# if best_run:
#     print("\nBest Run Metrics from Wandb Summary:")
#     print(json.dumps(best_run.summary, indent=2))

#     # You can perform further verification or usage of the best run here

"""# Trying to fix det.metrics (founded: we have to use .box first)"""

# def create_dummy_model():
#     """
#     Create a dummy YOLOv8 model for testing purposes.

#     Returns:
#         YOLO: Dummy YOLOv8 model.
#     """
#     print("Creating a dummy YOLOv8 model...")

#     # Create a dummy YOLOv8 model
#     model = YOLO('yolov8n.yaml')  # Use a small YOLOv8 configuration file

#     print("Performing dummy training...")

#     # Perform dummy training (optional)
#     model.train(data='/kaggle/working/data.yaml', epochs=1, imgsz=320, batch=8)

#     print("Dummy YOLOv8 model created and trained.")

#     return model

# def evaluate_yolov8(model, dataset_type='val'):
#     """
#     Evaluate the YOLOv8 model on the validation/test dataset.

#     Args:
#         model: YOLOv8 model.
#         dataset_type (str): Type of dataset to evaluate on ('val' or 'test').

#     Returns:
#         dict: Evaluation metrics.
#     """
#     print(f"Evaluating the model on the {dataset_type} set...")

#     try:
#         results = model.val(data='/kaggle/working/data.yaml', split=dataset_type, imgsz=320, batch=8)
#         print("Model evaluation completed successfully.")
#     except Exception as e:
#         print(f"Error occurred during model evaluation: {str(e)}")
#         return None

#     try:
#         # Access the DetMetrics object
#         metrics = results.box

#         avg_metrics = {
#             'val_precision': metrics.mean_results()[0],  # Mean precision
#             'val_recall': metrics.mean_results()[1],     # Mean recall
#             'val_mAP_0.5': metrics.map50,                # mAP@0.5
#             'val_mAP_0.5_0.95': metrics.map,             # mAP@0.5:0.95
#             'val_fitness': metrics.fitness,              # Fitness score
#         }

#         print(f"Evaluation metrics: {avg_metrics}")
#         return avg_metrics

#     except AttributeError as e:
#         print(f"AttributeError occurred while accessing metrics: {str(e)}")
#         return None

#     except Exception as e:
#         print(f"Unexpected error occurred while processing metrics: {str(e)}")
#         return None

# def main():
#     print("Starting the dummy YOLOv8 model evaluation.")

#     # Create a dummy YOLOv8 model
#     dummy_model = create_dummy_model()

#     # Evaluate the dummy model
#     metrics = evaluate_yolov8(dummy_model)

#     if metrics is not None:
#         print("Evaluation completed successfully.")
#     else:
#         print("Evaluation failed.")

#     print("Dummy YOLOv8 model evaluation finished.")

# if __name__ == '__main__':
#     main()